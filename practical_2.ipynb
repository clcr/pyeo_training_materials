{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "2019-04-12 10:32:35,555: INFO: ****PROCESSING START****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenacity, Planet and Multiprocessing are required for Planet data downloading\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 200\n",
    "\n",
    "import sys\n",
    "sys.path.append(r\"/opt/pyeo\")\n",
    "sen2cor_path = r\"/opt/Sen2Cor-02.05.05-Linux64/bin/L2A_Process\"\n",
    "import pyeo.core as pyeo\n",
    "import joblib\n",
    "import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "pyeo.init_log(\"training_log.log\")\n",
    "\n",
    "def show_satellite_image(image_path):\n",
    "    img = gdal.Open(image_path)\n",
    "    array = img.GetVirtualMemArray()\n",
    "    if len(array.shape) >= 3:\n",
    "        img_view = array.transpose([1,2,0])\n",
    "    else:\n",
    "        img_view = array\n",
    "    plt.imshow(img_view)\n",
    "    img_view = None\n",
    "    array = None\n",
    "    img = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6\n",
    "\n",
    "We have an image downloaded now, but we will need a model.\n",
    "\n",
    "To build a model, we will need training data. This is a list of **classes**, each with associated **features**. For Pyeo, the **features** are pixel values and the **classes** are the change class associated with that pixel.\n",
    "\n",
    "We have prepared an example training data set, to classify eight classes land use in Brazil. To view this, open sig.csv from the browser.\n",
    "\n",
    "Each row of this represents a pixel of a certain class. The first value in each row is the class of that pixel, with each value after that being the Sentinel 2 blue, green, red and NIR value of that pixel.\n",
    "\n",
    "For example, the first row represents class 5, with pixel values of blue = 122.0, green=325.0, red = 254.0 and NIR = 2033.0\n",
    "\n",
    "To create your own .csv file, you can use ArcGIS \n",
    "\n",
    "You can create a scikit-learn Random Forest model using Pyeo with the following function:\n",
    "\n",
    "```python\n",
    "pyeo.create_model_from_signatures(\n",
    "    sig_csv_path,\n",
    "    model_out_path\n",
    ")\n",
    "```\n",
    "\n",
    "`sig_csv_path` is the path to a .csv file, like sig.csv.\n",
    "\n",
    "`model_out_path` is where the model will be created.\n",
    "\n",
    "This creates a model that can be used to classify Sentinel-2 pixels\n",
    "\n",
    "# Exercise 5: Use sig.csv to create a trained model\n",
    "**NOTE:** When you save the model, name it (something).pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pyeo.classify_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Load the model back into Pyeo and explore it\n",
    "\n",
    "The model we created in step six a Python object, saved as a file. In Python, this is called *pickling*.\n",
    "\n",
    "To load a pickled object back into Python, use the following snippet:\n",
    "\n",
    "```python\n",
    "object = joblib.load(\"path/to/pickle.pkl\")\n",
    "```\n",
    "The model you have saved is a Scikit Learn ExtraTreesClassifier object: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n",
    "\n",
    "(If most of that page does not make sense to you, do not panic. It doesn't make much sense to me either.)\n",
    "\n",
    "Normally when you use Pyeo, it handles loading the model for you - see the next step. But knowing how to explore a model if you have been handed it blindly is a very useful skill.\n",
    "\n",
    "# Exercise 6: Load the model you just created. Using the page above and dir(), find the feature with the most importance to the classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Prepare the image you downloaded for classification\n",
    "\n",
    "Each band in a Sentinel 2 image comes as an individual image. To prepare for classification, Pyeo stacks all relevent bands into a single image, using the following function:\n",
    "\n",
    "```python\n",
    "pyeo.stack_sentinel_2_bands(\n",
    "    safe_dir,\n",
    "    out_image_path\n",
    ")\n",
    "```\n",
    "\n",
    "By deafult, this stackes every 10m band in the .SAFE dir; bands 2,3,4 and 8A and saves the output at out_image_path.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "pyeo.stack_sentinel_2_bands(\n",
    "    \"images/L2A_MSIL2A_lotsofnumbers.SAFE\",\n",
    "    \"images/merged_image.tif\"\n",
    ")\n",
    "```\n",
    "\n",
    "will create a four-band geotiff at images/stacked_image.tif\n",
    "\n",
    "## Exercise 7: Stack the level 2 image you preprocessed yesterday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Classify the image\n",
    "\n",
    "At last! For this step, the pyeo function you will need is this one:\n",
    "\n",
    "```python\n",
    "pyeo.classify_image(\n",
    "    image_path,\n",
    "    model_path,\n",
    "    output_path\n",
    ")\n",
    "```\n",
    "\n",
    "The example:\n",
    "\n",
    "```python\n",
    "pyeo.classify_image(\n",
    "    \"images/my_good_image.tif\",\n",
    "    \"my_model.pkl\",\n",
    "    \"outputs/my_good_outputs.tif\"\n",
    ")\n",
    "```\n",
    "\n",
    "will create a raster named my_good_outputs.tif, in the folder outputs\n",
    "\n",
    "This creates a raster of classes; each pixel is a class.\n",
    "\n",
    "## Exercise 8: Use pyeo.classify_image to classify the image you stacked in the previous step\n",
    "This will take betweem 15 and 40 minutes, depending on how many other people are using this machine.\n",
    "If you want, use `show_satellite_image` to view your output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Cloud masking\n",
    "\n",
    "The image we are using for this practical has a large amount of cloud cover.\n",
    "\n",
    "We can use a combination of two cloud masking techniques - fmask and sen2cor - to eliminate these clouds from our analysis. To do this, we create a .msk file.\n",
    "\n",
    "Pyeo makes the assumption that any .msk is the cloudmask for a .tif file with the same name; for example, `merged_image.msk` is the cloudmask for `merged_image.tif`\n",
    "\n",
    "```python\n",
    "pyeo.create_mask_from_sen2cor_and_fmask(\n",
    "    l1_dir,\n",
    "    l2_dir,\n",
    "    out_mask_path,\n",
    ")\n",
    "```\n",
    "\n",
    "`l1_dir` and `l2_dir` are the paths to the level 1C and 2A .SAFE files.\n",
    "\n",
    "Pyeo's masking algorithm requires both level 1 and level 2 Sentinel data; you have to provide paths to both .SAFE files for this function to work.\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "pyeo.create_mask_from_sen2cor_and_fmask(\n",
    "    \"S2B_MSIL1C_20190104T074319_N0207_R092_T36MZE_20190104T095459.SAFE\",\n",
    "    \"S2B_MSIL2A_20190104T074319_N0207_R092_T36MZE_20190104T095459.SAFE\",\n",
    "    \"images/merged_image.msk\"\n",
    ")\n",
    "```\n",
    "\n",
    "creates a cloudmask for `merged_image.tif` using the two .SAFE files. Note they have **almost** identical names, except for MSIL1C / MSIL2A\n",
    "\n",
    "The .msk file is just a geotiff with two values; 0 (cloud) and 1 (not cloud).\n",
    "\n",
    "## Exercise 9: Create a cloudmask for the image you downloaded.\n",
    "You can view the mask using `show_satellite_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Classify using the mask\n",
    "\n",
    "`pyeo.classify_image` has an extra option. If you add the augment use_mask=True, Pyeo will look for a .msk corresponding to the .tif you want to classify:\n",
    "\n",
    "```python\n",
    "pyeo.classify_image(\n",
    "    image_path,\n",
    "    model_path,\n",
    "    output_path,\n",
    "    apply_mask=True\n",
    ")\n",
    "```\n",
    "\n",
    "## Exercise 10: Re-run classify_image with `apply_mask = True.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations; you have downloaded, preprocessed, masked and classified a Sentinel 2 image using Pyeo.\n",
    "\n",
    "There are a number of helper functions in Pyeo to make performing these actions on large amounts of data easier. Each of the functions you have called in this training session are wrapped in functions that call them on entire directories of images at a time; for example `atmospheric_correction(in_dir, out_dir, sen2cor_path)` calls Sen2Cor on every image in in_dir and saves each of the results in out_dir. Similarly, `preprocess_s2_data(in_dir, out_dir, epsg)` wraps steps 8 and 9 together, along with an optional step to reproject to an EPSG of your choice.\n",
    "\n",
    "# Further exercise: change detection\n",
    "\n",
    "There are many methods of change detection. The one we favour in Leicester at present is this:\n",
    "\n",
    "* Stack two images together to form an 8 band image (old BGRI, new BGRI)\n",
    "* Classify this using a model trained on 8 features per class\n",
    "\n",
    "When you have reached this step, let an instructor know and we will provide you wil a pre-trained model. Using what you have learned in this workshop, along with the following pyeo function that stacks an old and a new image into a single geotif:\n",
    "\n",
    "```python\n",
    "pyeo.stack_old_and_new_images(\n",
    "    old_image_path,\n",
    "    new_image_path,\n",
    "    out_dir,\n",
    "    create_combined_mask=True,\n",
    ")\n",
    "```\n",
    "\n",
    "Classify change between your two images.\n",
    "\n",
    "You will need to download and preprocess another image from the Kinangop region.\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
